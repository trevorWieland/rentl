# Audit Log

Running record of all task audits, demo runs, and spec audits.
Future auditors: check this log for regressions and patterns.

---

- **Task 2** (round 1): PASS — Benchmark schema module, rubric/report models, and unit validation/round-trip coverage align with plan and standards.
- **Task 3** (round 1): FAIL — Eval-set task marked complete prematurely; committed manifest/slice artifacts and parse-flow coverage are missing, and default parser IDs fail schema validation on KSRE-style filenames.
- **Task 3** (round 2): FAIL — Parser ID normalization fix is verified, but manifest hashes are invalid placeholders and the configured demo slice does not meet required mixed-content coverage.
- **Task 2** (round 2): PASS — Task 2 schema contract remains satisfied in current tree; strict typed Pydantic models and benchmark schema tests still pass (32/32).
- **Task 4** (round 1): FAIL — Integration test uses direct pytest assertions instead of required BDD Given/When/Then structure for integration tier scenarios.
- **Task 4** (round 2): FAIL — BDD conversion is present but broken: async `When` step is never awaited, so integration scenarios do not execute and required prompt/result assertions fail.
- **Task 4** (round 3): FAIL — Metadata assertions in Task 4 unit/integration tests are partially no-op because expected-value checks were accidentally placed inside comments after `# type: ignore`.
- **Task 4** (round 4): PASS — Metadata assertions now enforce exact `mtl_baseline`/`model` values in unit and BDD integration tests, and targeted Task 4 suites pass (8/8).
- **Task 5** (round 1): FAIL — Judge integration BDD steps are unbound (`StepDefinitionNotFoundError`), and Task 5 test coverage misses randomized head-to-head remapping/per-dimension winner guarantees.
- **Task 4** (round 5): PASS — Re-audit confirms Task 4 implementation and BDD/unit coverage remain compliant; `pytest -q tests/unit/benchmark/test_mtl_baseline.py tests/integration/benchmark/test_mtl_baseline_flow.py` passes (8/8).
- **Task 5** (round 2): PASS — Task 5 fix items are implemented (BDD table steps bind, randomized head-to-head remapping is covered, per-dimension winners are enforced), and `pytest -q tests/unit/benchmark/test_judge.py tests/integration/benchmark/test_judge_flow.py` passes (23/23).
- **Task 8** (round 1): FAIL — Task was marked complete after adding unit report tests only; required benchmark CLI integration/quality coverage is missing, and new assertions codify a known head-to-head winner-mapping defect.
- **Task 7** (round 1): FAIL — Benchmark CLI remains placeholder-based (MTL reused as rentl), scoring-mode/reference handling is not implemented end-to-end, and report head-to-head winner aggregation compares incompatible label types.
- **Task 6** (round 1): PASS — `BenchmarkReportBuilder` and `format_report_summary` satisfy Task 6 scope (dimension aggregates, head-to-head rates, report assembly/serialization path, and unit coverage); `pytest -q tests/unit/benchmark/test_report_generation.py` passes (12/12).
- **Task 7** (round 2): FAIL — Winner-slot aggregation fix is verified, but scoring-mode still violates CLI/report contract (hyphenated values rejected and fallback can report `reference_based` while using no references).
- **Task 6** (round 2): PASS — Re-audit confirms Task 6 report generation contract remains satisfied (aggregation, head-to-head summary, report formatting, and JSON serialization path); resolved winner-slot mapping signpost behavior is verified in code/tests.
- **Task 7** (round 3): PASS — Verified fallback metadata fix: when reference-based mode falls back, `actual_scoring_mode` is set to `reference_free` before report generation (`services/rentl-cli/src/rentl_cli/main.py:2454`, `services/rentl-cli/src/rentl_cli/main.py:2515`).
- **Task 8** (round 2): FAIL — Benchmark CLI integration BDD coverage is broken at collection time because `test_cli_command.py` references a non-existent feature path (`FileNotFoundError`), so Task 8 cannot remain checked off.
- **Task 7** (round 4): FAIL — Benchmark CLI still uses placeholder pipeline wiring (`rentl_translations = mtl_translations`), so the task contract to run the real rentl pipeline is not met.
- **Task 3** (round 3): PASS — Eval-set downloader/parser artifacts and contracts remain valid; resolved hash and slice signposts are verified in code, and Task 3 suites pass (`34/34`).
- **Task 4** (round 6): FAIL — Output loader work is correct, but Task 4 still leaves regressions: stale CLI integration mocks reference deleted `MTLBaselineGenerator`, dead placeholder benchmark code remains, and judge API removals broke integration BDD scenarios.
- **Task 4** (round 7): FAIL — `test_cli_command` remains broken after Task 4 fixes (`RubricJudge` monkeypatch targets a deleted symbol, and BDD assertions still expect legacy benchmark behavior instead of the Task 7 rewrite stub output).
- **Task 4** (round 8): PASS — Task 4 implementation is now aligned with current benchmark stub behavior and remains compliant with output-loader/dead-code-removal scope; targeted Task 4 suites pass (`pytest -q tests/unit/benchmark/test_output_loader.py tests/integration/benchmark/test_cli_command.py tests/integration/benchmark/test_judge_flow.py` → `21 passed`).
- **Task 5** (round 3): FAIL — Task was checked off before completing required unit-test migration; `tests/unit/benchmark/test_judge.py` still contains 8 skipped legacy isolated-scoring tests targeting removed APIs.
- **Task 5** (round 4): PASS — Pairwise-only judge migration is fully active: no skipped legacy isolated-scoring coverage remains, and Task 5 suites pass (`pytest -q tests/unit/benchmark/test_judge.py tests/integration/benchmark/test_judge_flow.py` → `13 passed`).
- **Task 6** (round 3): FAIL — Task 6 remains incomplete: report generation still requires caller-supplied `overall_ranking` and `compute_elo_ratings` crashes with `ZeroDivisionError` for zero-comparison summaries.
- **Task 6** (round 4): PASS — Verified fix commit `aa21171`: `compute_elo_ratings` skips zero-comparison pairs and `build_report` derives `overall_ranking` from Elo; validation passed (`pytest -q tests/unit/benchmark/test_report.py` → `20 passed`, `uv run pyright packages/rentl-core/src/rentl_core/benchmark/report.py tests/unit/benchmark/test_report.py` → `0 errors`).

- **Demo** (run 1): FAIL — Step 1 command syntax mismatch: demo.md documents `--eval-set katawa-shoujo` (kebab-case) but CLI expects `katawa_shoujo` (snake_case). No normalization in CLI. Task 9 added to fix.
- **Task 9** (round 1): PASS — Verified eval-set kebab→snake normalization in benchmark download CLI (`services/rentl-cli/src/rentl_cli/main.py:1131`), with integration BDD coverage and passing target suite (`pytest -q tests/integration/benchmark/test_cli_command.py` → `2 passed`).
- **Demo** (run 2): PASS — Step 1 download executes successfully. Steps 2-5 (running rentl with different models/configs and comparing outputs) cannot be executed without API keys and would require significant runtime. Comparison mechanics validated via quality test instead. Not a failure per run-demo protocol.
- **Spec Audit** (round 1): FAIL — Rubric `3/4/2/5/3`, non-negotiables PASS, demo latest PASS (partial), fix-now count 4.

---

**Architecture revision** (resolve-blockers, 2026-02-10): spec.md, plan.md, demo.md, signposts.md rewritten. Original design embedded pipeline execution inside benchmark command and hardcoded JP→EN 2-system comparison. Revised to: benchmark is a pure comparison tool taking 2+ rentl run output files, head-to-head only (no isolated scoring), N-way all-pairs with Elo, language-agnostic. MTL baseline is just a translate-only rentl run. Tasks 1+3 remain done; Tasks 2/4/5/6/7/8 require rework under new plan. Old Task 4 (MTL baseline generator) replaced with output loader + dead code removal.

- **Task 8** (round 3): FAIL — The quality benchmark fix is incomplete: with documented quality env vars set, `tests/quality/benchmark/test_benchmark_quality.py` still fails before judge calls because CLI requires `OPENAI_API_KEY`/`ANTHROPIC_API_KEY` while the test only passes `RENTL_QUALITY_API_KEY`.
- **Task 9** (round 2): PASS — Re-audit confirms kebab-case eval-set normalization is still implemented in benchmark download and covered by integration BDD scenario; demo Step 1 command remains aligned (`--eval-set katawa-shoujo`).
- **Task 7** (round 5): PASS — Candidate naming interface now matches spec/demo: `benchmark compare` accepts `--candidate-names` in comma-separated format and routes parsed names through comparison (`services/rentl-cli/src/rentl_cli/main.py:1235`, `services/rentl-cli/src/rentl_cli/main.py:1260`), with quality scenario invocation aligned (`tests/quality/benchmark/test_benchmark_quality.py:156`).
- **Task 7** (round 6): FAIL — Parallelized compare execution is present, but progress accounting is incorrect under out-of-order completion because `completed=index + 1` is tied to task index rather than finished-task count (`services/rentl-cli/src/rentl_cli/main.py:1417`).
- **Demo** (run 3): PASS — Step 1 verified working after recent changes. Steps 2-5 validated via quality test. Full verification gate passes (805 unit + 81 integration + 5 quality tests).
- **Spec Audit** (round 2): FAIL — Rubric `4/4/3/5/3`, non-negotiables PASS, demo latest PASS, fix-now count 3 (progress accounting regression, stale `_run_benchmark_async` dead path, quality winner-label assertion mismatch).
- **Demo** (run 4): PASS — Step 1 executes successfully. Steps 2-5 validated via quality test with real LLMs. Full verification gate passes (805 unit + 81 integration + 5 quality tests).
- **Spec Audit** (round 3): FAIL — Rubric `4/4/3/5/3`, non-negotiables PASS, demo latest PASS (run 4), fix-now count 3 (progress monotonicity, stale `_run_benchmark_async` dead path, quality winner-label assertion mismatch).
- **Demo** (run 5): PASS — Step 1 executes successfully. Steps 2-5 validated via quality test with real LLMs. Full verification gate passes (805 unit + 81 integration + 5 quality tests).
- **Spec Audit** (round 4): FAIL — Rubric `4/4/2/5/3`, non-negotiables PASS, demo latest PASS (run 5), fix-now count 4 (progress monotonicity regression, stale `_run_benchmark_async` dead path, quality winner-label assertion mismatch, missing integration coverage for `benchmark compare` CLI flow).
- **Demo** (run 6): PASS — Step 1 executes successfully. Steps 2-5 validated via quality test with real LLMs. Full verification gate passes (805 unit + 81 integration + 5 quality tests).
- **Spec Audit** (round 5): FAIL — Rubric `4/4/2/5/3`, non-negotiables PASS, demo latest PASS (run 6), fix-now count 4 (progress monotonicity regression, stale `_run_benchmark_async` dead path, quality winner-label assertion mismatch, missing integration coverage for `benchmark compare` CLI flow).
- **Demo** (run 7): PASS — Step 1 executes successfully. Steps 2-5 validated via quality test with real LLMs. Full verification gate passes (805 unit + 81 integration + 5 quality tests).
- **Spec Audit** (round 6): FAIL — Rubric `4/4/2/5/3`, non-negotiables PASS, demo latest PASS (run 7), fix-now count 4 (progress monotonicity regression, stale `_run_benchmark_async` dead path, quality winner-label assertion mismatch, missing integration coverage for `benchmark compare` CLI flow).
- **Task 9** (round 3): PASS — Re-audit of Task 9 commit `6cdabd0` confirms kebab-case eval-set normalization in benchmark download is implemented and covered (`services/rentl-cli/src/rentl_cli/main.py:1131`, `tests/integration/benchmark/test_cli_command.py:102`), with targeted scenario passing (`pytest -q tests/integration/benchmark/test_cli_command.py -k kebab`).
- **Task 9** (round 4): PASS — Verified kebab→snake eval-set normalization is still implemented in benchmark download (`services/rentl-cli/src/rentl_cli/main.py:1131`), aligned with demo command format, and covered by passing integration/unit checks (`pytest -q tests/integration/benchmark/test_cli_command.py -k "kebab or subcommand"`, `pytest -q tests/unit/benchmark/eval_sets/test_loader.py`).
- **Demo** (run 8): PASS — Step 1 executes successfully. Steps 2-5 validated via quality test with real LLMs. Full verification gate passes (805 unit + 83 integration + 5 quality tests).
- **Spec Audit** (round 7): FAIL — Rubric `5/5/4/5/5`, non-negotiables PASS, demo latest PASS (run 8), fix-now count 1 (integration eval-set download tests are not BDD).
- **Task 9** (round 5): PASS — Verified kebab→snake eval-set normalization remains correct in benchmark download (`services/rentl-cli/src/rentl_cli/main.py:1131`), with passing focused and full integration CLI coverage plus existing eval-set loader unit tests.
- **Demo** (run 9): PASS — Step 1 executes successfully. Steps 2-5 validated via quality test with real LLMs.
- **Spec Audit** (round 8): PASS — Rubric `5/5/5/5/5`, non-negotiables PASS, demo latest PASS (run 9), fix-now count 0.
- **Task 10** (round 1): FAIL — Endpoint-config migration is incomplete: override mode still hard-requires config parsing, remaining judge defaults stay hardcoded, and required integration/quality/demo follow-up updates were not implemented.
